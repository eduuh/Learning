### Azure cli

- [Instructor] Deploying storage accounts from the Azure portal is easily done. You can do most of what you need to do from the portal. However, for continuous deployment or DevOps, any of those kinds of automation processes, you're probably going to use command line, whether it's the Azure command line or PowerShow or similar. So let's go to a brand new Windows 10 installation and go and install the Azure command line, which in fact, is cross-platform. So in this case, you will have a Windows machine and we can navigate to the documentation page for the Azure CLI, command line interface. And you can see, in this document they do discuss installation on Mac, on Linux, and even with Docker. In my case, I'll use the Windows installer, click on the NSI link. So I click on save and once downloaded, run the NSI. If you accept the license agreement, click the box and click install, accept the UAC prompt, and once completed, click finish. So now we have a command line interface installed that's cross-platform. And so the commands we'll use now will work on Mac, Linux, and Windows. I'll launch my command prompt and the Azure CLI, the application name is just AZ. Now the first thing to do, is in fact, to log in. And it requires us to navigate to a web page, put in this code, log in with our credentials, and then the AZ command line interface will be authenticated. Copy this URL, paste it into my browser, go back to the command prompt, copy the code, and paste it into this text box here. I'll click on continue and then log in with the user that has access to my subscription. It says now, you have signed into the Microsoft Azure Cross-platform Command Line interface application on your device. So now we can close this window and go back to our command prompt. So here we see the command prompt is, in fact, now authenticated. It shows us cloud name, Azure Cloud, the subscription ID, and our user information. So now that we're logged in, let's use the command line. AZ -H first, shows us the help information. And there's multiple ways to use this command line. So first off, the command itself is AZ and then we can use things like group, for adding a resource group. We'll do that first. And then storage, for manipulating our storage account. So AZ storage, and then from there, we will be able to create a storage account. Now to make this more interesting, and certainly more interactive, there is an interactive mode for the AZ command. So I'll type AZ interactive and now it will use my console as an interactive application. We'll send some telemetry to help Microsoft improve the product. Here we have the command prompt now in interactive mode. So we would like to modify the storage account, in this case, by creating a resource group and then creating a storage account in that resource group. So first thing to do is AZ group, space, and then the group is the resource group and the command is create. How do I know that, -H and it says here, AZ group create, creates a new resource group. So for each command, you can use the -H to find additional information about what is available. So I'll say AZ group, create, you see the internet is called up, we now have auto-completion, some suggestions for what is valid. So first off, for creating a new resource group, we specify a location. This is West Europe for me, pick one closest to you. And then next up will be name for our resource group. And we'll do this as learn Azure CLI. Now there are more options and as you use the dash, you can see the information available as the Azure CLI fetches it off the help files. So I'll just run AZ group, resource group, create, use this name, and make the default location West Europe for this resource group. Press enter and it sends us back the JSON from the service saying yes, this has been submitted and the provisioning has succeeded. So for storage we can do similar. Now for storage we have account, blob, container, and so forth. We're going to work the top level first and manipulate an account. In particular, we'll create a new account. Now for that, of course, we're going to need a resource group. And I can type this out as --resource-group or I can use the short form -G for the group and we called it learn Azure CLI, -N for the name, this will be our storage account name, so that DNS name. This has to be unique. So AZ storage, account, create a new account in the resource group Azure CLI, call it learn Azure CLI, I don't have to write out the full domain name. And remember the location is also important, we have the default from the resource group, but we do have to specify it here. And now, a storage account does have a SKU, so the type of storage account is important. And so when I press enter, I'll get an error. So see here, it says, storage account create following arguments are required. In particular, the SKU. So this is the storage account type. Press the up arrow to get the same line, - SKU, sorry, --SKU, and auto-complete can give us some options. Now remember we have premium storage and standard storage for general purpose storage accounts. And for blob storage accounts, we would only have the tiering of our blobs, hot, cool, or archive. So in this case, I'll pick the standard, local, redundant storage. And there is sufficient information for the storage account to be created with several defaults. Press enter and there's the JSON being returned. Scroll up a little bit and you can see we have a lot of information. We have, of course, the name, learn Azure CLI. We have the endpoints that we would connect to for blob, file, queue, and table storage. Primary location, provisioning has succeeded. So when you're doing any scripting for any of your automation, the Azure CLI, especially being cross-platform, is a great way to have one set of scripts that run everywhere, for deploying whatever you need in Azure. In this case, the Azure storage account.

```bash


# create a storage account
az storage account create -g Learn_azure_cli -n learnazurecli -l westeurope --sku Standard_LRS

```

### Shared key authentication

- [Instructor] The simplest way to authenticate, and authorized access for connecting to a storage account, is something called Access Keys. Take a look in the Azure portal, select one of your storage accounts, and below the overview you'll see an access keys entry. Click on access keys, and notice on the right hand side there's quite a bit of detail. First, the storage account name, significant so you'll have a set of keys per storage account. You'll have two keys, key one and key two, so that you can rotate keys meaning if an application is using the first key, you can reset the second one, and then let the application switch to the second key, and then you can reset the first one. So that allows you to always have at least one active and valid key, for applications to use connecting to the storage account. Notice that the keys though are essentially your root access (mumbles) access, to the storage account so given one of these keys, you'll have full access and you can do anything. So first off, try to avoid using these keys, there are other means of authenticating. If you do have to use these keys for the types of access your application needs, try not to hard code these into your configuration files, whether it's app config, web config, rather place these keys in something like Azure Key Vault, and retrieve them from there as necessary. So as I mentioned try to install these key in the Azure Key Vault, and do not that you do need to regenerate them, especially when they've been disclosed or they are put at risk, you need to regenerate them. And so for that scenario you have two keys at all times, one that your application is using actively, another one that you can reset to regenerate, and you can swap them out. Another trap, try to document all and any services that use your storage account keys. It's not always obvious which services and even Azure services, might've used these keys for access to the storage account. So start early when you create a storage account, start making notes of who these keys have been shared with, for which applications and services would be dependent on them. So now that you've seen how to create a storage account, you can find the access keys, and this will be first way of authenticating, and authorizing essentially as a root user, for accessing the storage account.

### PowerShell

Selecting transcript lines in this section will navigate to timestamp in the video

- [Instructor] In addition to the Azure command-line interface, there is also PowerShell libraries, PowerShell commandlets to interact with Azure. So, let's install the Azure PowerShell commandlets. And create a resource group and a storage account, using PowerShell. I prefer to use PowerShell kit, I find it to be fairly reliable. There's a couple of things we need to do to get this to work. So, let's launch PowerShell. And do launch it as administrator. And, so, from here, we will run the commands to install PowerShell. There's a couple of prerequisites. One is to run as administrator. Another is your execution policy. So, Set-ExecutionPolicy to RemoteSigned. Confirm. And now let's go to the documentation page for the installation scripts. First, we need PowerShellGet. This lets us install PowerShell modules from the PowerShell gallery. Copy the script, paste. And, so, here we'll see whether we have that, PowerShellGet. Yes. Then install PowerShell itself. This is Install-Module, AzureRM, for Azure Resource Manager. Confirm, yes. This is for NuGet, we just need an updated version. And we do want to install from the PowerShell gallery. We are pointing to the right one. And here we can see the Azure storage commandlet being installed, among many other Azure Resource Manager commandlets. And there, the installation complete. Remember, we're running as administrator. So, set ExecutionPolicy to RemoteSigned. And, ultimately, the install module, AzureRM, is what's actually installing the Azure PowerShell commands for us. Back here, in the documentation, Import-Module, for actually using the commands. And there's, of course, plenty of documentation about how to use PowerShell to talk to Azure. In our case, we'll create a resource group and a new storage account. First thing to do, Login-AzureRm. Tab for auto-complete, resource manager account. Press Enter, and get prompted for our login. In this case, I used the interactive login. And we can see here, the subscription is listed that we're going to talk to now. So, first, New-AzureRmResourceGroup, Tab, for auto-complete. We're going to need a name. This is the name of our resource group. We need a location. And Enter. So, new resource group created. And now let's create a new storage account. So, New-AzureRmStorageAccount. Tab, for the first parameter, ResourceGroupName, the one we just created. And select, press Enter to copy, right-click to paste. The next parameter is the name of our storage account. Remember, this has to be unique, so we might have a failure if someone else already picked that. And the Sku for our storage account. Same as before, Standard Local Redundant Storage. And we need the location. Press Enter. And there, we have the details of our storage account, newly created. Just be sure it's there, we can say Get-AzureRmStorageAccount. Tab, for auto-completion. And it will list all the storage accounts available. There is the one we just created, learn_azure_powershell, with several others. To make the formatting easier, and learn a little bit more about PowerShell, in this case, I'll say Format-Table. We'll get a table output, and this will now list all of our storage accounts. So, when we ran the new command, we got only the the one return, saying what was newly created. When I say, Get-AzureRmStorageAccount, we get a list of all storage accounts, since I did not falter my query. So, now you've seen how to create a storage account using the portal, command-line interface, and PowerShell.

### Local emulator

- [Instructor] We've created Azure store accounts on the Azure cloud, but that requires a network connection. And sometimes you want to develop offline or in an isolated environment. And for that, there is the Azure storage emulator. This might make it possible for you to test and de-bug a little bit faster, or at least work in an isolated environment until you're ready to deploy. The storage emulator is not 100% compatible with the Azure API in the cloud. There are more features in the cloud than available locally, but it'll get you most of the way there. In the documentation, there is an installer available. I'll click on stand alone installer, run. And if you accept the license agreement, check the box, install, and finish. Now the storage emulator does need storage, and it uses SQL server. And for that, we'll use SQL Server Express LocalDB. It's a nice and small installation. So from the documentation page for SQL Server downloads, go to express edition. Run, this is a small network installer. But instead of installing, we'll ask for the ISO. So from the download media, we'll ask just for LocalDB. Put it in my downloads folder and download. So its nice and small, simple installation. Go to our downloads folder. There's LocalDB, run the installer, click next. If you accept the agreement, check the radio button. Click next, and install. And that's probably the quickest, smallest SQL server instance one can get our hands on. We'll click finish, run the storage emulator, and a command prompt will show up. And so it found our LocalDB deployment, and it will use that then for its persistence. Now that the storage emulator is running, we can connect to it using the storage explorer or with our client libraries, as if it's the cloud.

### Blobs

- [Instructor] Azure storage blobs offer unstructured storage in the cloud. It's the general purpose storage. It's comparable with Amazon Web Services S3 and we have three types of blobs in Azure Storage. One is called block blobs, and this is what you would expect from a blob. You can upload the whole thing at once and you can make changes to parts of it, blocks within a blob. We have a page blob, which is what we use for virtual machine disks, and we have an append blob, which is append only. We can access Azure Storage through multiple client libraries, of course. .NET, Java, Node.js, and so forth. And if none of those fit your needs, then you can talk to the REST API directly. Now, the use cases for block blobs are manyfold and just think of it as your file system in the cloud. General storage with one significant consideration: We do have a tiering option. So, if we pick a storage account that is made specifically for blobs, so not a general purpose storage account, then we can choose hot, cool, or archive storage tiering. And so, we can optimize our costs based on whether we want frequent, high-performance access to a blob or whether we want infrequent access to it, or perhaps just want to store it long term and potentially, never access it. So, one of the key things to consider for blobs and when you design your solution is that a single storage account will have, for blobs, multiple containers. So, container is that first part of the naming, essentially. So, when you think of a storage account as a DNS endpoint that you connect to, a container's going to be the first thing that you name after the DNS name. It sort of looks like a folder in which you place your blobs. So, an example here, we have an account. Within their account, we have pictures and movies. So, pictures is one container, movies is another container. And it's just a way to structure your data and it'll have performance implications we'll talk about later. So, block blobs, just bear in mind, block blobs, an individual blob can have up to 50,000 blocks, so pieces within it. There's a difference between block blobs and append blobs in terms of the total size that you get. They can both have up to 50,000 blocks. But for block blobs, you can upload or update blocks up to 100 megs each. So, can get a total size just under five terabyte. But for append blobs, your block size is limited to four megabyte each. So, when you're appending 50K, 100K at a time, you're not going to make it to the maximum 195 gig. But if you chunk into full four megabyte blocks, then you can reach that maximum size. Page blobs are up to one terabyte, but that's for use with virtual machines. And that number is increasing as virtual machines demand more storage. So, one of the thing we have to consider for blobs, just like in file systems, is how do we deal with concurrency? We are going to end up with multiple clients wanting to talk to the same blob. So, one option is optimistic concurrency. So, if you have some control in your client or there is a low likelihood of you competing for access for an individual blob, you could just hope and then check at the time when you make a change that you're still making a change to an object as you remember it, or as you've last seen it. For that to work, we use the identity of an object, something called an ETag. So, an ETag is an identifier for the object that includes its stite, so a moment in time identifier. And so, when the identifier is the last time I read a blob, I know the blob hasn't changed. So, when I'm trying to make an update to a blob, if I read it first, think about it, and then update it, during that time I was thinking about it, the ETag must not change, and so I know the blob I'm updating is the one I did my processing on. Another option is pessimistic concurrency, where you just assume there's going to be contention and you want access to a blob to some degree of exclusivity. And it's called the lease. Now, the name for it is a lock, and we'll take a look at how to acquire a lease so that you can have exclusive access to one of the blobs to the degree that your application needs. So, for all of this to work, there's going to be quite a few types, but those that we often forget to consider are these three. In particular, the BlobRequestOptions. So, we're going to need that to add specific criteria when we talk to the blob storage service. And OperationContext, that helps us actually deal with logging and things like that that is carried along as we interact with the servers over time. So, we'll find look at append blobs. They're the simplest to work with. We'll have a new blob created and add to the end of the blob every time we interact with it. So, there are many use cases for this and when it comes to performance, certain algorithms are designed to simply add to an existing data stream. So, whether you're dealing with Telemetry, or logs like Syslog, or media streams that keep coming at you, you probably just want to append those to an existing block of data. And so, the blocks in an append blob can only ever be added to the end. They're up to four megabyte in size, gets you a maximum size of 195 gig. But remember your 50,000 block limit. So, you have to design your application to append blocks onto a blob in such a way that you're not going to run out of your maximum number of blocks. So, if it's a log file, you probably want to rotate your log files. And so, in the same way, on that same scheme of saying, "Here's a blob. "I'm going to write 10, 15, 20,000 blocks to it "and maybe that's an hour worth of logging.". And so, the next hour will be a new blob and you start appending to the new one, keeping you inside your maximum size limit and your number of blocks limit. The second type of blob we're looking at is the block blob and this is practically random access. So, we can add or update blocks within a blob and we essentially control the structure within that block by deciding the size of our blocks and giving them ID's. So, a block can be up to 100 megabyte and you can get a pretty large blob from that, up to 50,000 blocks of 100 megabyte each gets you just under five terabyte. Now, given the size of these blocks and given that we are going over the network, you may want to consider using an MD5 hash so that the hash is computed on the client. It's uploaded, the server will check it, and will commit a block only if the MD5 hash actually match. Now, speaking of commit, we can do parallel uploads of multiple blocks. And so, we have the opportunity to parallelize a lot of our network traffic and then right at the end, we choose to commit multiple blocks or a list of blocks. And so, if we upload five, 10, 15, we have to re-upload some of them or some of them change, we end up with a list of uncommitted blocks and then we choose which of those to commit. And so, an update process can become quite elaborate, but it means that you can parallelize tasks, you can retry, and then once you're ready, you select the set of blocks that you want to actually commit to that blob.

## Tables

- [Instructor] Azure storage tables is a data store for structured data in the NoSQL sense of structured data. So we store entities with key value or key attribute pairs associated with those entities and it is schemaless in that you can decide your key attribute pair collections and they can vary from one entity to another so the store is essentially schemaless. We use this a lot for something that has a bit more structure than say a blob, so any document or structured entity that you would like to store, tables would be a great place to do that so that you can query based on the contents or the keys in those key attribute pairs. As your storage tables can do this at a very large scale and at a much lower cost than a traditional relational database management system and it is relatively fast compared to RDBMSs at that very high scale. Now you don't get structured query language, it's not meant for relational data so you have a NoSQL database that's schemaless for structured data. Azure storage has been there from the beginning of Azure and tables from the early days as well and so it's a very mature platform and is now seeing an upgrade becoming available as part of Cosmos DB. So Cosmos DB is still in preview but it's an interesting database model in that it is multimodal so as a single service we get different data models so it supports the API for DocumentDB, MongoDB, Table API that we're talking about now, even a Graph API. So some of the limitations that we have in Azure storage tables have been alleviated or new options are available under Cosmos DB. At this time it's still in preview, but the Table API is compatible so we can move existing table applications over into Cosmos DB and just get the benefits. The basic structure of how we use Azure storage tables come as a hierarchy in that we have a storage account, this is the DNS name that we target when we connect to Azure and then within that storage account we have tables so tables are essentially connections of entities and then within each table and entity is a collection of properties of name-value pairs. So this will look like, if you think in terms of classes, an object with properties and the object kind of equivalent to an entity here and the objects in the collection kind of equivalent to a table here. So hierarchly we'll connect to an Azure storage account and then from there, in the table service, we will connect to a table, query a table, add entities to it and then manipulate individual entities by manipulating their properties in turn.
  Skip to section

### Queues

- [Instructor] Azure Storage Queues is essentially a queuing service built on Azure Storage. It's one of the fundamental storage features of Azure. And we use it for essentially email between applications. If you're familiar with Microsoft Message Queuing, or MQSeries, or Service Bus, then you already know Azure Storage queues. Now, bear in mind Azure Storage Queues are a lot simpler, much more of a fundamental service than MSMQ, MQSeries, or even Service Bus. Those are very rich in their APIs, where a Storage queue is pretty straightforward. Now, if you want to use Storage queues you have many different client libraries. And if you find a language that doesn't have a client library you could still talk to the Azure Storage queues using the REST API directly. So, what are Azure Storage queues good for? Well, they're great in many different use cases. In general terms we will talk about decoupling components. The idea of taking two components instead of them communicating with each other directly, to rather place a queue in-between them so that the service requesting some action or submitting some data can do so to a queue, so middleman that can accept the message or the request without actually doing the processing. And then service providing the actual capability or processing the data will take messages of that queues as fast as it can. But won't be overwhelmed with a deluge of messages or requests. And it won't necessarily have to be available right at the moment where the client submits its request. By decoupling components by placing a queue in-between them we get load smoothing, so you can send a lot of messages into a queue and have the queue grow in length without overwhelming the processor. Then, similarly, we can add some resilience by decoupling components in this way by having momentary blips unavailability of think processing messages being essentially invisible to the client submitting messages onto the queue. So, from the Microsoft architecture guidance there is actually a very good example that we use a lot. The idea is it's like a web app as an example, and consider how much traffic might be driven at that web app. So whether you're amazon.com or otherwise you're going to have a lot of requests coming in for service, so it could be retail check out scenarios, it could be video processing requirements, just generating thumbnails, but the web application itself has a lot of requests for work. But the client doesn't necessarily want to wait for that. So the client interacting with the web app wants to have a nice interactive experience. And so the client submitting a request through the web app expects an immediate response. So the web app will place the request. Let's say a check out request for a shopping cart into a queue, and then tell the client we're working on it. So the client has a nice interactive experience, but the work goes into a queue. And then we have a background task, in this case a web chore, or a worker-all in the classic model. Where we have the background task working as fast as possible working through all of the messages in the queue. Our focus is here where we're decoupling the web application from the background task. As the WebJob becomes temporarily unavailable, even just for a moment, the web app won't be affected by it. The web app can continue to submit messages into the queue. And even if the web app starts sending a lot of messages simply faster than the WebJob can consume, then we still have the ability to load smooth and that the queue will just grow in size. Once the WebJob becomes available or if it's just been very busy when we add resources it will then continue to process messages out of that queue at its maximum rate. So neither its availability nor its processing rate is directed going to affect the web application. This also means that we can scale these two components independently. As we have more clients we can scale our web application independent from the background task. We can also scale our background task, perhaps based on seasonality or as that queue length needs to be managed. If it gets too large or results don't appear quick enough for our clients, then we can actually choose to invest in more WebJob instances here and process the queue more quickly. So this is one example of how to use queues to decouple components. In this case decoupling a web application from the background task could be generating thumbnails, encoding videos, checking out shopping carts, that sort of thing. Let's also take a look at enterprise integration patterns. The website enterpriseintegrationpatterns.com, there's a lot of examples from what we would do in message-based systems. And many of those examples apply to what we might use Storage queues for in the cloud.

### Queues

- [Instructor] Azure Storage Queues is essentially a queuing service built on Azure Storage. It's one of the fundamental storage features of Azure. And we use it for essentially email between applications. If you're familiar with Microsoft Message Queuing, or MQSeries, or Service Bus, then you already know Azure Storage queues. Now, bear in mind Azure Storage Queues are a lot simpler, much more of a fundamental service than MSMQ, MQSeries, or even Service Bus. Those are very rich in their APIs, where a Storage queue is pretty straightforward. Now, if you want to use Storage queues you have many different client libraries. And if you find a language that doesn't have a client library you could still talk to the Azure Storage queues using the REST API directly. So, what are Azure Storage queues good for? Well, they're great in many different use cases. In general terms we will talk about decoupling components. The idea of taking two components instead of them communicating with each other directly, to rather place a queue in-between them so that the service requesting some action or submitting some data can do so to a queue, so middleman that can accept the message or the request without actually doing the processing. And then service providing the actual capability or processing the data will take messages of that queues as fast as it can. But won't be overwhelmed with a deluge of messages or requests. And it won't necessarily have to be available right at the moment where the client submits its request. By decoupling components by placing a queue in-between them we get load smoothing, so you can send a lot of messages into a queue and have the queue grow in length without overwhelming the processor. Then, similarly, we can add some resilience by decoupling components in this way by having momentary blips unavailability of think processing messages being essentially invisible to the client submitting messages onto the queue. So, from the Microsoft architecture guidance there is actually a very good example that we use a lot. The idea is it's like a web app as an example, and consider how much traffic might be driven at that web app. So whether you're amazon.com or otherwise you're going to have a lot of requests coming in for service, so it could be retail check out scenarios, it could be video processing requirements, just generating thumbnails, but the web application itself has a lot of requests for work. But the client doesn't necessarily want to wait for that. So the client interacting with the web app wants to have a nice interactive experience. And so the client submitting a request through the web app expects an immediate response. So the web app will place the request. Let's say a check out request for a shopping cart into a queue, and then tell the client we're working on it. So the client has a nice interactive experience, but the work goes into a queue. And then we have a background task, in this case a web chore, or a worker-all in the classic model. Where we have the background task working as fast as possible working through all of the messages in the queue. Our focus is here where we're decoupling the web application from the background task. As the WebJob becomes temporarily unavailable, even just for a moment, the web app won't be affected by it. The web app can continue to submit messages into the queue. And even if the web app starts sending a lot of messages simply faster than the WebJob can consume, then we still have the ability to load smooth and that the queue will just grow in size. Once the WebJob becomes available or if it's just been very busy when we add resources it will then continue to process messages out of that queue at its maximum rate. So neither its availability nor its processing rate is directed going to affect the web application. This also means that we can scale these two components independently. As we have more clients we can scale our web application independent from the background task. We can also scale our background task, perhaps based on seasonality or as that queue length needs to be managed. If it gets too large or results don't appear quick enough for our clients, then we can actually choose to invest in more WebJob instances here and process the queue more quickly. So this is one example of how to use queues to decouple components. In this case decoupling a web application from the background task could be generating thumbnails, encoding videos, checking out shopping carts, that sort of thing. Let's also take a look at enterprise integration patterns. The website enterpriseintegrationpatterns.com, there's a lot of examples from what we would do in message-based systems. And many of those examples apply to what we might use Storage queues for in the cloud.

### Files

- [Teacher] With Azure Storage Accounts we can very easily create files, or file shares. You might know them as a network share. And so, this allows us to migrate applications into Azure really easily if all they need is a file share. So, what do I mean with a file share? It might be used to the net use command, or the C-I-F-S CIFS shares that we can mount from Linux, or maybe it's a notation Backslash Backslash a server name followed by Backslash and a share name. So, if this looks familiar you are familiar with network shares or network file shares. And on an Azure storage account we can create one really easily. Now remember in Azure storage things are triple replicated at least in the local data center. So, when we create a file share, our data will already be redundantly stored, and so we get a highly available redundant file server from any general-purpose storage account. So practically what does this mean? Well, you're not getting a Windows Server, so there are some limitations. For example, you cannot Active Directory and its access control lists to create for example user folders. We access the network share with that key that we've previously seen. As there is basically one user name and password to get into a network share using the SMB protocol. Speaking of SMB, if you're connecting from a network connected PC, there are some version limits. So, for example you cannot use Windows XP or 2008. You need at least SMB 2.1 or 3.0. The Azure Storage file shares are implemented as an API that looks like SMB to us, so there is only a subset of the protocol implemented. So MSB 2.1 and 3 partly supported but the actual implementation is using Blobs and tables behind the scenes. So, we get a SMB interface and a REST API. So, this means even though there are some limitations, like not having Active Directory some other things being left out of the protocol, like short file names, alternate data streams. We do get two different ways to get to the same share, SMB, so the net use command or, our custom clients can use the REST API. So, using the REST API we can have a very familiar experience, kind of similar to the Blob API in that we can talk to a REST API to access files. And so, the real magic here is that when we migrate applications to Azure, applications that require file share, we can use a legacy application that currently uses file shares and build new features and new applications cloud-native we're using the REST API to get to the same underlying data. And the best part is that the REST API and SMB does work concurrently, they work at the same time. They even respect each other's locks so when you have a wright lock on a file using SMB the REST API will actually respect that. Now there is a catch, originally the network shares were not encrypted. So, when we connect to a network share in the old days the traffic was essentially in clear text. With SMB 3 we do get encryption. So, encryption between a client, and a network share, even it if you don't have IP second implemented. With a client that supports SMB 3 you can connect you can connect to an Azure share pretty much from anywhere, as long as the network ports are open, and authentication is successful. However, if you have a slightly older client, you can actually use SMB 2.1 clients. Such as older Windows clients, some Linux clients and such, from inside the Azure environment. So, if you have a retro machine you could connect to an Azure share, even if that client does not yet support the full SMB 3 encrypted scenario. But the boundary of Azure is where that limitation is then enforced. In that, you cannot send clear text SMB traffic from outside of Azure to a file share implemented by Azure storage. So, for your migration scenarios, just consider that and also for troubleshooting. Often people are on Windows XP clients, or server 2008 machines trying to connect to an Azure storage share, and they don't realize they're on too old a version SMB, not supporting encryption
- Skip to section

### Next steps

Selecting transcript lines in this section will navigate to timestamp in the video

- [Anton] Now that you've heard of Azure Storage capabilities, in general, the next step is to go in-depth. There are four more courses, each one taking a deep dive into Azure Storage, specifically, going into Azure Storage Blobs in-depth by looking at the API and the unique capabilities of blobs. Then there is Azure Storage Files, where Azure Storage looks like and behaves like a fileshare, but we can access those files either programmatically, through a REST API, as well as through traditional SMB or net use-style semantics. Then there is Azure Storage Queues, which is equivalent to email for applications. If you think in terms of a queuing system through which we can send and receive messages, similar to how we send and receive messages through an email mailbox. And then there is Azure Storage Tables, which is how we manage structured data in Azure Storage. And I blink the use of a schemaless but structured datastore in the NoSQL-style of things. This is also the precursor to Cosmos DB, for which there's additional courses worth looking into. When you take a deep-dive look at Azure Storage, you'll come across the async and await keywords whenever you're using C#. This is because we'll almost always want to use a asynchronous operation when calling storage APIs. Async keyword is not a prerequisite for understanding Azure Storage, but in almost all cases, you'll have to consider the latency impact of accessing storage. And so whenever using C#, the async keyword is going to come up. And so I encourage you go take an in-depth look at the async keyword as well. Thank you for watching this course and good luck on your journey with Azure Storage.

Give feedback
0 notifications total
